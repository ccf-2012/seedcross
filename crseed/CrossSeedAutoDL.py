import json
import logging
import re
import requests
import time
from datetime import datetime  # 添加此行
import pytz  # 添加此行
from guessit import guessit
from urllib.parse import urlencode

from .torcategory import GuessCategoryUtils
from .tortitle import parseMovieName
from .models import CrossTorrent, TaskControl, SearchedHistory
from .torclient import SeedingTorrent

logger = logging.getLogger(__name__)

# logger.setLevel(logging.DEBUG)
# formatter = logging.Formatter('\n%(asctime)s - Module: %(module)s - Line: %(lineno)d - Message: %(message)s')
# file_handler = logging.FileHandler('CrossSeedAutoDL.log', encoding='utf8')
# file_handler.setFormatter(formatter)

# logger.addHandler(file_handler)


class Searcher:
    # 1 MibiByte == 1024^2 bytes
    MiB = 1024**2
    # max size difference (in bytes) in order to account for extra or missing files, eg. nfo files
    # size_differences_strictness = {True: 0, False: 2 * MiB}
    # # max_size_difference = size_differences_strictness[ARGS.strict_size]
    # max_size_difference = size_differences_strictness[False]

    # torznab categories: 2000 for movies, 5000 for TV. This dict is for matching against the (str) types generated by 'guessit'
    category_types = {'movie': 2000, 'episode': 5000}

    def __init__(self, process_param):
        self.search_results = []
        self.process_param = process_param
        # TODO: shoud this be configurable
        self.max_size_difference = process_param.max_size_difference

    def search(self, local_release_data, log, guess_cat=''):
        if local_release_data['size'] is None:
            s = 'Skipped: Could not get proper filesize data'
            logger.info(s)
            log.message(s)
            return []

        search_query = local_release_data['guessed_data']['title']
        if local_release_data['guessed_data'].get('year') is not None:
            search_query += ' ' + str(
                local_release_data['guessed_data']['year'])

        if self.process_param.jackett_prowlarr == 0:
            search_url = self._get_jackett_search_url(search_query,
                                                      local_release_data, guess_cat)
        elif self.process_param.jackett_prowlarr == 1:
            search_url = self._get_prowlarr_search_url(search_query,
                                                       local_release_data, guess_cat)
        else:
            log.message('This is impossible.')
            return []

        if not search_url:
            log.message(
                'Skip, No indexer configured for [ {} ] category'.format(guess_cat))
            return []

        logger.info(search_url)

        resp = None
        for n in range(2):
            try:
                resp = requests.get(search_url, local_release_data)
                break
            except requests.exceptions.ReadTimeout:
                if n == 0:
                    s = 'ERROR: Connection timed out. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)
            except requests.exceptions.ConnectionError:
                if n == 0:
                    s = 'ERROR: Connection failed. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)

        if not resp:
            s = 'ERROR: No response, check the Jackett/Prowlarr setting.'
            log.message(s, error_abort=1)
            return []
        try:
            resp_json = resp.json()
        except json.decoder.JSONDecodeError as e:
            print('Json decode error. Incident logged')
            s = 'ERROR: Json decode Error. Response text: ' + resp.text
            logger.info(s)
            logger.exception(e)
            log.message(s)
            return []

        if self.process_param.jackett_prowlarr == 0:
            if resp_json['Indexers'] == []:
                info = 'ERROR: No results found due to incorrectly input indexer names ({}). Check ' \
                       'your spelling/capitalization. Are they added to Jackett? Exiting...'.format(
                           self.process_param.trackers)
                print(info)
                logger.info(info)
                log.message(info, error_abort=1)
                return []
            result_json = resp_json['Results']
        elif self.process_param.jackett_prowlarr == 1:
            result_json = resp_json

        trim_result = self.loadToIndexResult(result_json)
        return self._get_matching_results(local_release_data, trim_result, log)

    def _get_prowlarr_search_url(self, search_query, local_release_data, category=''):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v1/search?'

        main_params = {
            'type': 'search',
            'apikey': self.process_param.jackett_api_key,
            'query': search_query
        }

        if categoryMovieTV(category):
            optional_params = {
                'category':
                Searcher.category_types[local_release_data['guessed_data']
                                        ['type']],
                'season':
                local_release_data['guessed_data'].get('season'),
                'episode':
                local_release_data['guessed_data'].get('episode')
            }

            for param, arg in optional_params.items():
                if arg is not None:
                    main_params[param] = arg

        indexerUrl = None
        if self.process_param.category_indexers and category:
            if categoryMovieTV(category):
                if self.process_param.indexer_movietv.strip():
                    idlist = self.process_param.indexer_movietv.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryMusic(category):
                if self.process_param.indexer_music.strip():
                    idlist = self.process_param.indexer_music.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryAudio(category):
                if self.process_param.indexer_audio.strip():
                    idlist = self.process_param.indexer_audio.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryEBook(category):
                if self.process_param.indexer_ebook.strip():
                    idlist = self.process_param.indexer_ebook.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryOther(category):
                if self.process_param.indexer_other.strip():
                    idlist = self.process_param.indexer_other.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            if indexerUrl:
                return base_url + urlencode(main_params) + '&'+indexerUrl
            else:
                return None
        else:
            if self.process_param.trackers.strip():
                idlist = self.process_param.trackers.split(',')
                indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
                return base_url + urlencode(main_params) + '&'+indexerUrl
            else:
                return base_url + urlencode(main_params)

    # construct final search url

    def _get_jackett_search_url(self, search_query, local_release_data, category=''):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v2.0/indexers/all/results?'

        main_params = {
            'apikey': self.process_param.jackett_api_key,
            'Query': search_query
        }

        optional_params = {}
        if categoryMovieTV(category):
            optional_params = {
                'Category[]':
                Searcher.category_types[local_release_data['guessed_data']
                                        ['type']],
                'season':
                local_release_data['guessed_data'].get('season'),
                'episode':
                local_release_data['guessed_data'].get('episode')
            }

        indexerUrl = None
        if self.process_param.category_indexers and category:
            if categoryMovieTV(category):
                if self.process_param.indexer_movietv.strip():
                    indexerUrl = self.process_param.indexer_movietv
            elif categoryMusic(category):
                if self.process_param.indexer_music.strip():
                    indexerUrl = self.process_param.indexer_music
            elif categoryAudio(category):
                if self.process_param.indexer_audio.strip():
                    indexerUrl = self.process_param.indexer_audio
            elif categoryEBook(category):
                if self.process_param.indexer_ebook.strip():
                    indexerUrl = self.process_param.indexer_ebook
            elif categoryOther(category):
                if self.process_param.indexer_other.strip():
                    indexerUrl = self.process_param.indexer_other

            if indexerUrl:
                optional_params['Tracker[]'] = indexerUrl
            else:
                return None
        else:
            if self.process_param.trackers.strip():
                indexerUrl = self.process_param.trackers
                optional_params['Tracker[]'] = indexerUrl

        for param, arg in optional_params.items():
            if arg is not None:
                main_params[param] = arg

        return base_url + urlencode(main_params)

    # some titles in jackett search results get extra data appended in square brackets,
    # ie. 'Movie.Name.720p.x264 [Golden Popcorn / 720p / x264]'
    @staticmethod
    def _reformat_release_name(release_name):
        release_name_re = r'^(.+?)( \[.*/.*\])?$'

        match = re.search(release_name_re, release_name, re.IGNORECASE)
        if match:
            return match.group(1)

        logger.info(f'"{release_name}" name could not be trimmed down')
        return release_name

    def loadToIndexResult(self, search_results):
        index_results = []

        for result in search_results:
            if self.process_param.jackett_prowlarr == 0:
                if 'Link' not in result:
                    return []
                r = IndexResult(
                    indexer=result['Tracker'],
                    categories=result['Category'],
                    title=self._reformat_release_name(result['Title']),
                    downloadUrl=result['Link'],
                    infoUrl=result['Details'],
                    size=result['Size'],
                    imdbId=result['Imdb'],
                    TrackerType=result['TrackerType'],
                )
            elif self.process_param.jackett_prowlarr == 1:
                if 'downloadUrl' not in result:
                    return []
                r = IndexResult(
                    indexer=result['indexer'],
                    categories=result['categories'],
                    title=self._reformat_release_name(result['title']),
                    downloadUrl=result['downloadUrl'],
                    infoUrl=result['infoUrl'],
                    size=result['size'],
                    imdbId=result['imdbId'],
                    TrackerType='private',
                )

            # skipping result if its from a public tracker/indexer
            if r.TrackerType != "public":
                index_results.append(r)

        return index_results

    def _get_matching_results(self, local_release_data, index_result, log):
        matching_results = []
        # print(f'Parsing { len(self.search_results) } results. ', end='')

        for result in index_result:
            max_size_difference = self.max_size_difference
            # older torrents' sizes in blutopia are are slightly off
            if result.indexer == 'Blutopia':
                max_size_difference *= 2
            if (max_size_difference > 2048) and (result.size < 1500 * self.MiB):
                max_size_difference = 2048

            m = re.match(local_release_data['tracker'], result.indexer, re.I)
            if m:
                continue

            if abs(result.size -
                   local_release_data['size']) <= max_size_difference:
                matching_results.append(result)

        s = f'{ len(matching_results) } matched of { len(index_result) } results.'

        logger.info(s)
        log.message(s)

        return matching_results


class IndexResult():
    def __init__(self, indexer, categories, title, downloadUrl, infoUrl, size,
                 imdbId, TrackerType):
        self.indexer = indexer
        self.categories = categories
        self.title = title
        self.downloadUrl = downloadUrl
        self.infoUrl = infoUrl
        self.size = size
        self.imdbId = imdbId
        self.TrackerType = TrackerType


def categoryMovieTV(category):
    return category in ['TV', 'MovieEncode', 'MovieWebdl', 'MovieBDMV', 'MovieBDMV4K', 'MovieDVD', 'MovieWeb4K', 'MovieRemux', 'HDTV', 'Movie4K', 'MV']


def categoryMusic(category):
    return category in ['Music']


def categoryEBook(category):
    return category in ['eBook']


def categoryAudio(category):
    return category in ['Audio']


def categoryOther(category):
    return category in ['Other']


def genSearchKeyword(basename, size, tracker, log, skip_CJK=True, parseTitle=''):
    local_release_data = {
        'basename': basename,
        'size': size,
        'guessed_data': guessit(basename),
        'tracker': tracker
    }

    # # TODO: use parseTitle
    if parseTitle:
        local_release_data['guessed_data']['title'] = parseTitle

    if local_release_data['guessed_data'].get('title') is None:
        s = 'Skipped: Could not get title from filename: {}'.format(
            local_release_data['basename'])

        logger.info(s)
        log.message(s)

        return []

    if skip_CJK:
        if re.search(r'[\u4e00-\u9fa5\u3041-\u30fc]',
                     local_release_data['guessed_data']['title']):
            s = 'Skipped: contains CJK characters in title: {}'.format(
                local_release_data['basename'])
            logger.info(s)
            log.message(s)

            return []

    # log.message('Query title: ' + local_release_data['guessed_data']['title'])
    return local_release_data


def isPreviouslySearched(torName):
    return SearchedHistory.objects.filter(name=torName).exists()


def saveSearchedTorrent(st):
    newSt = SearchedHistory()
    newSt.hash = st.torrent_hash
    newSt.name = st.name
    newSt.size = st.size
    newSt.location = st.save_path
    newSt.tracker = st.tracker
    newSt.added_date = st.added_date
    newSt.save()
    return newSt
    # newSt.status = status


def saveCrossedTorrent(st, searchTor):
    newCt = CrossTorrent()
    newCt.hash = st.torrent_hash
    newCt.name = st.name
    newCt.size = st.size
    newCt.location = st.save_path
    newCt.crossed_with = searchTor
    newCt.tracker = st.tracker
    newCt.added_date = st.added_date
    newCt.save()
    return newCt


def downloadResult(dlclient, result, localTor, log):
    # Jackett if result['Link'] is None:
    if result.downloadUrl is None:
        s = 'Skipped: - Skipping release (no download link): ' + localTor.name
        logger.info(s)
        log.message(s)
        return None
    s = 'Grabbing release: ' + result.title
    logger.info(s)
    log.message(s)
    # Jackett return dlclient.addTorrentUrl(result['Link'], localTor.save_path)
    # print(result)
    ret = dlclient.addTorrentUrl(result.downloadUrl, localTor.save_path, result.title, result.indexer)
    return ret


def checkTaskCanclled():
    taskctrl = TaskControl.objects.all().last()
    if not taskctrl:
        return False
    else:
        return taskctrl.cancel_task


def iterTorrents(dlclient, process_param, log):
    FlowControlLimitCount = process_param.fc_count  # count limit per button press
    FlowControlInterval = process_param.fc_interval  # seconds between query
    log.status(flow_limit=FlowControlLimitCount)

    log.message('Loading torrents in the client')
    torList = dlclient.loadTorrents()
    log.status(total_in_client=len(torList))

    for_count = 0
    query_count = 0
    for localTor in torList:
        if query_count >= FlowControlLimitCount:
            return

        for_count += 1
        log.status(progress=for_count)
        if checkTaskCanclled() or log.abort():
            return

        if isPreviouslySearched(localTor.name):
            continue

        dbSearchTor = saveSearchedTorrent(localTor)
        catutil = GuessCategoryUtils()
        cat, group = catutil.guessByName(localTor.name)

        log.message('Torrent: [ {} ] {}'.format(cat, localTor.name))
        # # TODO: use parseTitle
        parseTitle, parseYear, parseSeason, parseEpisode, cntitle = parseMovieName(localTor.name)
        log.message('Searching: {} {} {} {}'.format(parseTitle, parseYear, parseSeason, parseEpisode))
        searchData = genSearchKeyword(localTor.name, localTor.size,
                                      localTor.tracker, log, process_param.skip_CJK, parseTitle)
        # # use guessit
        # searchData = genSearchKeyword(localTor.name, localTor.size,
        #                               localTor.tracker, log, process_param.skip_CJK)

        if not searchData:
            continue

        query_count += 1
        log.status(progress=for_count, query_count=query_count)

        searcher = Searcher(process_param)
        matchingResults = searcher.search(searchData, log, guess_cat=cat)
        log.inc(match_count=len(matchingResults))
        for result in matchingResults:
            if checkTaskCanclled() or log.abort():
                return
            time.sleep(10)
            st = downloadResult(dlclient, result, localTor, log)
            if st:
                # logger.info(f'- Torrent added: {result.title}')
                log.inc(download_count=1)
                # log.message('Torrent Added: ' + result.title)
                saveCrossedTorrent(st, dbSearchTor)


        time.sleep(FlowControlInterval)


def test_download_result(dlclient, process_param, log):
    localTor = SeedingTorrent(
        torrent_hash='testhash123',
        name='Test Movie',
        size=1500 * Searcher.MiB,
        save_path='/path/to/save',
        tracker='TestTracker',
        added_date=datetime.now(pytz.utc),  # 需要导入 datetime 和 pytz
        status='seeding'  # 添加 status 字段
    )
    result = IndexResult(
        indexer='TestIndexer',
        categories=['Movie'],
        title='About Endlessness 2019 1080p BluRay DTS x264-HDS',
        downloadUrl='http://192.168.5.8:9117/dl/mteamtp/?jackett_apikey=NqTlUwWml2WU1UQ05FcjhrcHd3&file=About+Endlessness+2019+1080p+BluRay+DTS+x264-HDS',
        infoUrl='https://kp.m-team.cc/detail/624943',
        size=5150 * Searcher.MiB,
        imdbId='',
        TrackerType='private'
    )
    st = downloadResult(dlclient, result, localTor, log)
    if st:
        print(f'- Success added: {result.title}')
        logger.info(f'- Success added: {result.title}')
        log.inc(download_count=1)
        log.message('Added: ' + result.title)
    else:
        print(f'- Failed to add: {result.title}')
        logger.info(f'- Failed to add: {result.title}')
        log.message('Failed to add: ' + result.title, error_abort=1)

