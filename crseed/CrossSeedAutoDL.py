import json
import logging
import re
import requests
import time
from guessit import guessit
from urllib.parse import urlencode

from .torcategory import GuessCategoryUtils
from .tortitle import parseMovieName
from .models import CrossTorrent, TaskControl, SearchedHistory

logger = logging.getLogger(__name__)

# logger.setLevel(logging.DEBUG)
# formatter = logging.Formatter('\n%(asctime)s - Module: %(module)s - Line: %(lineno)d - Message: %(message)s')
# file_handler = logging.FileHandler('CrossSeedAutoDL.log', encoding='utf8')
# file_handler.setFormatter(formatter)

# logger.addHandler(file_handler)


class Searcher:
    # 1 MibiByte == 1024^2 bytes
    MiB = 1024**2
    # max size difference (in bytes) in order to account for extra or missing files, eg. nfo files
    size_differences_strictness = {True: 0, False: 2 * MiB}
    # max_size_difference = size_differences_strictness[ARGS.strict_size]
    max_size_difference = size_differences_strictness[False]

    # torznab categories: 2000 for movies, 5000 for TV. This dict is for matching against the (str) types generated by 'guessit'
    category_types = {'movie': 2000, 'episode': 5000}

    def __init__(self, process_param):
        self.search_results = []
        self.process_param = process_param
        # TODO: shoud this be configurable
        # self.max_size_difference = process_param.max_size_difference

    def search(self, local_release_data, log, guess_cat=''):
        if local_release_data['size'] is None:
            s = 'Skipped: Could not get proper filesize data'
            logger.info(s)
            log.message(s)
            return []

        search_query = local_release_data['guessed_data']['title']
        if local_release_data['guessed_data'].get('year') is not None:
            search_query += ' ' + str(
                local_release_data['guessed_data']['year'])

        if self.process_param.jackett_prowlarr == 0:
            search_url = self._get_jackett_search_url(search_query,
                                                      local_release_data, guess_cat)
        elif self.process_param.jackett_prowlarr == 1:
            search_url = self._get_prowlarr_search_url(search_query,
                                                       local_release_data, guess_cat)
        else:
            log.message('This is impossible.')
            return []

        if not search_url:
            log.message(
                'Skip, No indexer configured for [ {} ] category'.format(guess_cat))
            return []

        logger.info(search_url)

        resp = None
        for n in range(2):
            try:
                resp = requests.get(search_url, local_release_data)
                break
            except requests.exceptions.ReadTimeout:
                if n == 0:
                    s = 'ERROR: Connection timed out. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)
            except requests.exceptions.ConnectionError:
                if n == 0:
                    s = 'ERROR: Connection failed. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)

        if not resp:
            s = 'ERROR: No response, check the Jackett/Prowlarr setting.'
            log.message(s, error_abort=1)
            return []
        try:
            resp_json = resp.json()
        except json.decoder.JSONDecodeError as e:
            print('Json decode error. Incident logged')
            s = 'ERROR: Json decode Error. Response text: ' + resp.text
            logger.info(s)
            logger.exception(e)
            log.message(s)
            return []

        if self.process_param.jackett_prowlarr == 0:
            if resp_json['Indexers'] == []:
                info = 'ERROR: No results found due to incorrectly input indexer names ({}). Check ' \
                       'your spelling/capitalization. Are they added to Jackett? Exiting...'.format(
                           self.process_param.trackers)
                print(info)
                logger.info(info)
                log.message(info, error_abort=1)
                return []
            result_json = resp_json['Results']
        elif self.process_param.jackett_prowlarr == 1:
            result_json = resp_json

        trim_result = self.loadToIndexResult(result_json)
        return self._get_matching_results(local_release_data, trim_result, log)

    def _get_prowlarr_search_url(self, search_query, local_release_data, category=''):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v1/search?'

        main_params = {
            'type': 'search',
            'apikey': self.process_param.jackett_api_key,
            'query': search_query
        }

        if categoryMovieTV(category):
            optional_params = {
                'category':
                Searcher.category_types[local_release_data['guessed_data']
                                        ['type']],
                'season':
                local_release_data['guessed_data'].get('season'),
                'episode':
                local_release_data['guessed_data'].get('episode')
            }

            for param, arg in optional_params.items():
                if arg is not None:
                    main_params[param] = arg

        indexerUrl = None
        if self.process_param.category_indexers and category:
            if categoryMovieTV(category):
                if self.process_param.indexer_movietv.strip():
                    idlist = self.process_param.indexer_movietv.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryMusic(category):
                if self.process_param.indexer_music.strip():
                    idlist = self.process_param.indexer_music.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryAudio(category):
                if self.process_param.indexer_audio.strip():
                    idlist = self.process_param.indexer_audio.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryEBook(category):
                if self.process_param.indexer_ebook.strip():
                    idlist = self.process_param.indexer_ebook.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            elif categoryOther(category):
                if self.process_param.indexer_other.strip():
                    idlist = self.process_param.indexer_other.split(',')
                    indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
            if indexerUrl:
                return base_url + urlencode(main_params) + '&'+indexerUrl
            else:
                return None
        else:
            if self.process_param.trackers.strip():
                idlist = self.process_param.trackers.split(',')
                indexerUrl = urlencode({'indexerIds': idlist}, doseq=True)
                return base_url + urlencode(main_params) + '&'+indexerUrl
            else:
                return base_url + urlencode(main_params)

    # construct final search url

    def _get_jackett_search_url(self, search_query, local_release_data, category=''):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v2.0/indexers/all/results?'

        main_params = {
            'apikey': self.process_param.jackett_api_key,
            'Query': search_query
        }

        optional_params = {}
        if categoryMovieTV(category):
            optional_params = {
                'Category[]':
                Searcher.category_types[local_release_data['guessed_data']
                                        ['type']],
                'season':
                local_release_data['guessed_data'].get('season'),
                'episode':
                local_release_data['guessed_data'].get('episode')
            }

        indexerUrl = None
        if self.process_param.category_indexers and category:
            if categoryMovieTV(category):
                if self.process_param.indexer_movietv.strip():
                    indexerUrl = self.process_param.indexer_movietv
            elif categoryMusic(category):
                if self.process_param.indexer_music.strip():
                    indexerUrl = self.process_param.indexer_music
            elif categoryAudio(category):
                if self.process_param.indexer_audio.strip():
                    indexerUrl = self.process_param.indexer_audio
            elif categoryEBook(category):
                if self.process_param.indexer_ebook.strip():
                    indexerUrl = self.process_param.indexer_ebook
            elif categoryOther(category):
                if self.process_param.indexer_other.strip():
                    indexerUrl = self.process_param.indexer_other

            if indexerUrl:
                optional_params['Tracker[]'] = indexerUrl
            else:
                return None
        else:
            if self.process_param.trackers.strip():
                indexerUrl = self.process_param.trackers
                optional_params['Tracker[]'] = indexerUrl

        for param, arg in optional_params.items():
            if arg is not None:
                main_params[param] = arg

        return base_url + urlencode(main_params)

    # some titles in jackett search results get extra data appended in square brackets,
    # ie. 'Movie.Name.720p.x264 [Golden Popcorn / 720p / x264]'
    @staticmethod
    def _reformat_release_name(release_name):
        release_name_re = r'^(.+?)( \[.*/.*\])?$'

        match = re.search(release_name_re, release_name, re.IGNORECASE)
        if match:
            return match.group(1)

        logger.info(f'"{release_name}" name could not be trimmed down')
        return release_name

    def loadToIndexResult(self, search_results):
        index_results = []

        for result in search_results:
            if self.process_param.jackett_prowlarr == 0:
                r = IndexResult(
                    indexer=result['Tracker'],
                    categories=result['Category'],
                    title=self._reformat_release_name(result['Title']),
                    downloadUrl=result['Link'],
                    infoUrl=result['Details'],
                    size=result['Size'],
                    imdbId=result['Imdb'],
                )
            elif self.process_param.jackett_prowlarr == 1:
                r = IndexResult(
                    indexer=result['indexer'],
                    categories=result['categories'],
                    title=self._reformat_release_name(result['title']),
                    downloadUrl=result['downloadUrl'],
                    infoUrl=result['infoUrl'],
                    size=result['size'],
                    imdbId=result['imdbId'],
                )

            index_results.append(r)
        return index_results

    def _get_matching_results(self, local_release_data, index_result, log):
        matching_results = []
        # print(f'Parsing { len(self.search_results) } results. ', end='')

        for result in index_result:
            max_size_difference = self.max_size_difference
            # older torrents' sizes in blutopia are are slightly off
            if result.indexer == 'Blutopia':
                max_size_difference *= 2
            if (max_size_difference > 2048) and (result.size < 1500 * self.MiB):
                max_size_difference = 2048

            m = re.match(local_release_data['tracker'], result.indexer, re.I)
            if m:
                continue

            if abs(result.size -
                   local_release_data['size']) <= max_size_difference:
                matching_results.append(result)

        s = f'{ len(matching_results) } matched of { len(index_result) } results.'

        logger.info(s)
        log.message(s)

        return matching_results


class IndexResult():
    def __init__(self, indexer, categories, title, downloadUrl, infoUrl, size,
                 imdbId):
        self.indexer = indexer
        self.categories = categories
        self.title = title
        self.downloadUrl = downloadUrl
        self.infoUrl = infoUrl
        self.size = size
        self.imdbId = imdbId


def categoryMovieTV(category):
    return category in ['TV', 'MovieEncode', 'MovieWebdl', 'MovieBDMV', 'MovieBDMV4K', 'MovieDVD', 'MovieWeb4K', 'MovieRemux', 'HDTV', 'Movie4K', 'MV']


def categoryMusic(category):
    return category in ['Music']


def categoryEBook(category):
    return category in ['eBook']


def categoryAudio(category):
    return category in ['Audio']


def categoryOther(category):
    return category in ['Other']


def genSearchKeyword(basename, size, tracker, log, skip_CJK=True, parseTitle=''):
    local_release_data = {
        'basename': basename,
        'size': size,
        'guessed_data': guessit(basename),
        'tracker': tracker
    }

    # # TODO: use parseTitle
    if parseTitle:
        local_release_data['guessed_data']['title'] = parseTitle

    if local_release_data['guessed_data'].get('title') is None:
        s = 'Skipped: Could not get title from filename: {}'.format(
            local_release_data['basename'])

        logger.info(s)
        log.message(s)

        return []

    if skip_CJK:
        if re.search(r'[\u4e00-\u9fa5\u3041-\u30fc]',
                     local_release_data['guessed_data']['title']):
            s = 'Skipped: contains CJK characters in title: {}'.format(
                local_release_data['basename'])
            logger.info(s)
            log.message(s)

            return []

    # log.message('Query title: ' + local_release_data['guessed_data']['title'])
    return local_release_data


def isPreviouslySearched(torName):
    return SearchedHistory.objects.filter(name=torName).exists()


def saveSearchedTorrent(st):
    newSt = SearchedHistory()
    newSt.hash = st.torrent_hash
    newSt.name = st.name
    newSt.size = st.size
    newSt.location = st.save_path
    newSt.tracker = st.tracker
    newSt.added_date = st.added_date
    newSt.save()
    return newSt
    # newSt.status = status


def saveCrossedTorrent(st, searchTor):
    newCt = CrossTorrent()
    newCt.hash = st.torrent_hash
    newCt.name = st.name
    newCt.size = st.size
    newCt.location = st.save_path
    newCt.crossed_with = searchTor
    newCt.tracker = st.tracker
    newCt.added_date = st.added_date
    newCt.save()
    return newCt


def downloadResult(dlclient, result, localTor, log):

    # Jackett if result['Link'] is None:
    if result.downloadUrl is None:
        s = 'Skipped: - Skipping release (no download link): ' + localTor.name
        logger.info(s)
        log.message(s)
        return None
    s = 'Grabbing release: ' + result.title
    logger.info(s)
    log.message(s)
    # Jackett return dlclient.addTorrentUrl(result['Link'], localTor.save_path)
    ret = dlclient.addTorrentUrl(result.downloadUrl, localTor.save_path, result.title)
    return ret


def checkTaskCanclled():
    taskctrl = TaskControl.objects.all().last()
    if not taskctrl:
        return False
    else:
        return taskctrl.cancel_task


def iterTorrents(dlclient, process_param, log):
    FlowControlLimitCount = process_param.fc_count  # count limit per button press
    FlowControlInterval = process_param.fc_interval  # seconds between query
    log.status(flow_limit=FlowControlLimitCount)

    log.message('Loading torrents in the client')
    torList = dlclient.loadTorrents()
    log.status(total_in_client=len(torList))

    for_count = 0
    query_count = 0
    for localTor in torList:
        if query_count >= FlowControlLimitCount:
            return

        for_count += 1
        log.status(progress=for_count)
        if checkTaskCanclled() or log.abort():
            return

        if isPreviouslySearched(localTor.name):
            continue

        dbSearchTor = saveSearchedTorrent(localTor)
        catutil = GuessCategoryUtils()
        cat, group = catutil.guessByName(localTor.name)

        log.message('Torrent: [ {} ] {}'.format(cat, localTor.name))
        # # TODO: use parseTitle
        parseTitle, parseYear, parseSeason, parseEpisode, cntitle = parseMovieName(localTor.name)
        log.message('Searching: {} {} {} {}'.format(parseTitle, parseYear, parseSeason, parseEpisode))
        searchData = genSearchKeyword(localTor.name, localTor.size,
                                      localTor.tracker, log, process_param.skip_CJK, parseTitle)
        # # use guessit
        # searchData = genSearchKeyword(localTor.name, localTor.size,
        #                               localTor.tracker, log, process_param.skip_CJK)

        if not searchData:
            continue

        query_count += 1
        log.status(progress=for_count, query_count=query_count)

        searcher = Searcher(process_param)
        matchingResults = searcher.search(searchData, log, guess_cat=cat)
        log.inc(match_count=len(matchingResults))
        for result in matchingResults:
            if checkTaskCanclled() or log.abort():
                return
            time.sleep(10)
            st = downloadResult(dlclient, result, localTor, log)
            if st:
                print(f'- Success added: {result.title}')
                logger.info(f'- Success added: {result.title}')
                log.inc(download_count=1)
                log.message('Added: ' + result.title)
                saveCrossedTorrent(st, dbSearchTor)
            # else:
            #     log.message('Maybe existed: ' + localTor.name)

        time.sleep(FlowControlInterval)
