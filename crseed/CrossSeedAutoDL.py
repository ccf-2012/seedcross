from asyncio.streams import FlowControlMixin
import json
import logging
import os
import re
from django.utils import timezone
import requests
import shutil
import time
from guessit import guessit
from urllib.parse import urlencode
from .models import CrossTorrent, TaskControl, SearchedHistory

logger = logging.getLogger(__name__)

# logger.setLevel(logging.DEBUG)
# formatter = logging.Formatter('\n%(asctime)s - Module: %(module)s - Line: %(lineno)d - Message: %(message)s')
# file_handler = logging.FileHandler('CrossSeedAutoDL.log', encoding='utf8')
# file_handler.setFormatter(formatter)

# logger.addHandler(file_handler)


class Searcher:
    # 1 MibiByte == 1024^2 bytes
    MiB = 1024**2
    # max size difference (in bytes) in order to account for extra or missing files, eg. nfo files
    size_differences_strictness = {True: 0, False: 5 * MiB}
    # max_size_difference = size_differences_strictness[ARGS.strict_size]
    max_size_difference = size_differences_strictness[False]

    # torznab categories: 2000 for movies, 5000 for TV. This dict is for matching against the (str) types generated by 'guessit'
    category_types = {'movie': 2000, 'episode': 5000}

    def __init__(self, process_param):
        self.search_results = []
        self.process_param = process_param

    def search(self, local_release_data, log):
        if local_release_data['size'] is None:
            s = 'Skipped: Could not get proper filesize data'
            logger.info(s)
            log.message(s)
            return []

        search_query = local_release_data['guessed_data']['title']
        if local_release_data['guessed_data'].get('year') is not None:
            search_query += ' ' + str(
                local_release_data['guessed_data']['year'])

        if self.process_param.jackett_prowlarr == 0:
            search_url = self._get_jackett_search_url(search_query,
                                                      local_release_data)
        elif self.process_param.jackett_prowlarr == 1:
            search_url = self._get_prowlarr_search_url(search_query,
                                                       local_release_data)
        logger.info(search_url)

        resp = None
        for n in range(2):
            try:
                resp = requests.get(search_url, local_release_data)
                break
            except requests.exceptions.ReadTimeout:
                if n == 0:
                    s = 'ERROR: Connection timed out. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)
            except requests.exceptions.ConnectionError:
                if n == 0:
                    s = 'ERROR: Connection failed. Retrying once more.'
                    print(s)
                    log.message(s, error_abort=1)
                    time.sleep(self.process_param.delay)

        if not resp:
            s = 'ERROR: No response, check the Jackett/Prowlarr setting.'
            log.message(s, error_abort=1)
            return []
        try:
            resp_json = resp.json()
        except json.decoder.JSONDecodeError as e:
            print('Json decode error. Incident logged')
            s = 'ERROR: Json decode Error. Response text: ' + resp.text
            logger.info(s)
            logger.exception(e)
            log.message(s)
            return []

        if self.process_param.jackett_prowlarr == 0:
            if resp_json['Indexers'] == []:
                info = 'ERROR: No results found due to incorrectly input indexer names ({}). Check ' \
                       'your spelling/capitalization. Are they added to Jackett? Exiting...'.format(self.process_param.trackers)
                print(info)
                logger.info(info)
                log.message(info, error_abort=1)
                return []
            result_json = resp_json['Results']
        elif self.process_param.jackett_prowlarr == 1:
            result_json = resp_json

        # Jackett self.search_results = self._trim_results(resp_json['Results'])
        trim_result = self.loadToIndexResult(result_json)
        return self._get_matching_results(local_release_data, trim_result, log)

    def _get_prowlarr_search_url(self, search_query, local_release_data):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v1/search?'

        main_params = {
            'type': 'search',
            'apikey': self.process_param.jackett_api_key,
            'query': search_query
        }

        optional_params = {
            # Jackett 'Category[]':
            'category':
            Searcher.category_types[local_release_data['guessed_data']
                                    ['type']],
            'season':
            local_release_data['guessed_data'].get('season'),
            'episode':
            local_release_data['guessed_data'].get('episode')
        }

        for param, arg in optional_params.items():
            if arg is not None:
                main_params[param] = arg

        if self.process_param.trackers.strip():
            idlist = self.process_param.trackers.split(',')
            indexerUrl = urlencode({'indexerIds':idlist}, doseq=True)


        return base_url + urlencode(main_params)+ '&'+indexerUrl

    # construct final search url
    def _get_jackett_search_url(self, search_query, local_release_data):
        base_url = self.process_param.jackett_url.strip(
            '/') + '/api/v2.0/indexers/all/results?'

        main_params = {
            'apikey': self.process_param.jackett_api_key,
            'Query': search_query
        }

        optional_params = {
            'Category[]':
            Searcher.category_types[local_release_data['guessed_data']
                                    ['type']],
            'season':
            local_release_data['guessed_data'].get('season'),
            'episode':
            local_release_data['guessed_data'].get('episode')
        }
        if self.process_param.trackers.strip():
            optional_params['Tracker[]'] = self.process_param.trackers

        for param, arg in optional_params.items():
            if arg is not None:
                main_params[param] = arg

        return base_url + urlencode(main_params)

    # some titles in jackett search results get extra data appended in square brackets,
    # ie. 'Movie.Name.720p.x264 [Golden Popcorn / 720p / x264]'
    @staticmethod
    def _reformat_release_name(release_name):
        release_name_re = r'^(.+?)( \[.*/.*\])?$'

        match = re.search(release_name_re, release_name, re.IGNORECASE)
        if match:
            return match.group(1)

        logger.info(f'"{release_name}" name could not be trimmed down')
        return release_name

    def loadToIndexResult(self, search_results):
        index_results = []

        for result in search_results:
            if self.process_param.jackett_prowlarr == 0:
                r = IndexResult(
                    indexer=result['Tracker'],
                    categories=result['Category'],
                    title=self._reformat_release_name(result['Title']),
                    downloadUrl=result['Link'],
                    infoUrl=result['Details'],
                    size=result['Size'],
                    imdbId=result['Imdb'],
                )
            elif self.process_param.jackett_prowlarr == 1:
                r = IndexResult(
                    indexer=result['indexer'],
                    categories=result['categories'],
                    title=self._reformat_release_name(result['title']),
                    downloadUrl=result['downloadUrl'],
                    infoUrl=result['infoUrl'],
                    size=result['size'],
                    imdbId=result['imdbId'],
                )

            index_results.append(r)
        return index_results

    def _get_matching_results(self, local_release_data, index_result, log):
        matching_results = []
        # print(f'Parsing { len(self.search_results) } results. ', end='')

        for result in index_result:
            max_size_difference = self.max_size_difference
            # older torrents' sizes in blutopia are are slightly off
            if result.indexer == 'Blutopia':
                max_size_difference *= 2

            m = re.match(local_release_data['tracker'], result.indexer, re.I)
            if m:
                continue

            if abs(result.size -
                   local_release_data['size']) <= max_size_difference:
                matching_results.append(result)

        s = f'{ len(matching_results) } matched of { len(index_result) } results.'

        logger.info(s)
        log.message(s)

        return matching_results

    ###
    # def _save_results(self, local_release_data):
    #     search_results_path = os.path.join( os.path.dirname(os.path.abspath(__file__)), 'search_results.json' )
    #     target_dict = {'local_release_data': local_release_data, 'results': self.search_results}
    #
    #     with open(search_results_path, 'w', encoding='utf8') as f:
    #         json.dump([target_dict], f, indent=4)


class IndexResult():
    def __init__(self, indexer, categories, title, downloadUrl, infoUrl, size,
                 imdbId):
        self.indexer = indexer
        self.categories = categories
        self.title = title
        self.downloadUrl = downloadUrl
        self.infoUrl = infoUrl
        self.size = size
        self.imdbId = imdbId


def genSearchKeyword(basename, size, tracker, log):
    local_release_data = {
        'basename': basename,
        'size': size,
        'guessed_data': guessit(basename),
        'tracker': tracker
    }

    if local_release_data['guessed_data'].get('title') is None:
        s = 'Skipped: Could not get title from filename: {}'.format(
            local_release_data['basename'])

        logger.info(s)
        log.message(s)

        return []

    if re.search(r'[\u4e00-\u9fa5\u3041-\u30fc]',
                 local_release_data['guessed_data']['title']):
        s = 'Skipped: contains CJK characters in title: {}'.format(
            local_release_data['basename'])
        logger.info(s)
        log.message(s)

        return []

    return local_release_data


def isPreviouslySearched(torName):
    return SearchedHistory.objects.filter(name=torName).exists()


def saveSearchedTorrent(st):
    newSt = SearchedHistory()
    newSt.hash = st.torrent_hash
    newSt.name = st.name
    newSt.size = st.size
    newSt.location = st.save_path
    newSt.tracker = st.tracker
    newSt.added_date = st.added_date
    newSt.save()
    return newSt
    # newSt.status = status


def saveCrossedTorrent(st, searchTor):
    newCt = CrossTorrent()
    newCt.hash = st.torrent_hash
    newCt.name = st.name
    newCt.size = st.size
    newCt.location = st.save_path
    newCt.crossed_with = searchTor
    newCt.tracker = st.tracker
    newCt.added_date = st.added_date
    newCt.save()
    return newCt


def downloadResult(dlclient, result, localTor, log):

    #Jackett if result['Link'] is None:
    if result.downloadUrl is None:
        s = 'Skipped: - Skipping release (no download link): ' + localTor.name
        logger.info(s)
        log.message(s)
        return None
    s = 'Grabbing release: ' + localTor.name
    logger.info(s)
    log.message(s)
    # Jackett return dlclient.addTorrentUrl(result['Link'], localTor.save_path)
    return dlclient.addTorrentUrl(result.downloadUrl, localTor.save_path)


def checkTaskCanclled():
    taskctrl = TaskControl.objects.all().last()
    if not taskctrl:
        return False
    else:
        return taskctrl.cancel_task


def iterTorrents(dlclient, process_param, log):
    FlowControlLimitCount = process_param.fc_count  # count limit per button press
    FlowControlInterval = process_param.fc_interval  # seconds between query
    log.status(flow_limit=FlowControlLimitCount)

    log.message('Loading torrents in the client')
    torList = dlclient.loadTorrents()
    log.status(total_in_client=len(torList))

    for_count = 0
    query_count = 0
    for localTor in torList:

        for_count += 1
        log.status(progress=for_count)
        if checkTaskCanclled() or log.abort():
            return

        if isPreviouslySearched(localTor.name):
            continue

        dbSearchTor = saveSearchedTorrent(localTor)
        searchData = genSearchKeyword(localTor.name, localTor.size,
                                      localTor.tracker, log)
        if not searchData:
            continue

        query_count += 1
        log.status(progress=for_count, query_count=query_count)
        log.message('Searching: ' + searchData['guessed_data']['title'])
        if query_count >= FlowControlLimitCount:
            return
        searcher = Searcher(process_param)
        matchingResults = searcher.search(searchData, log)
        log.inc(match_count=len(matchingResults))
        # log.message('Found: %d Results' % (len(matchingResults)))
        for result in matchingResults:
            if checkTaskCanclled() or log.abort():
                return
            st = downloadResult(dlclient, result, localTor, log)
            if st:
                print(f'- Success added: {localTor.name}')
                logger.info(f'- Success added: {localTor.name}')
                log.inc(download_count=1)
                log.message('Added: ' + localTor.name)
                saveCrossedTorrent(st, dbSearchTor)
            else:
                log.message('Skiped: ' + localTor.name)

        time.sleep(FlowControlInterval)
